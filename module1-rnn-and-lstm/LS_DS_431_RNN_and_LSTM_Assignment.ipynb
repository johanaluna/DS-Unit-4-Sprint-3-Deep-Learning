{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"100-0.txt\"\n",
    "\n",
    "# text = (open(filename).read()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# with open('100-0.txt', 'r') as f:\n",
    "#     text = f.read().encode(encoding='UTF-8',errors='strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "fileObj = codecs.open( \"100-0.txt\", \"r\", \"utf-8\" )\n",
    "text = fileObj.read() \n",
    "# Returns a Unicode string from the UTF-8 bytes in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740053"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Characteres\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting Chars\n",
    "#set help to create a list with out duplicate values.\n",
    "chars = sorted(list(set(text)))\n",
    "# chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique Characteres\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = ['apple', 'banana', 'grapes', 'pear','apple']\n",
    "# for d, value in enumerate(my_list, 1):\n",
    "#     print(d, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars (index, character)\n",
    "# Save indices into a dictionary\n",
    "char_indices = dict((c,i) for i, c in enumerate(chars))\n",
    "\n",
    "#Enumerate help me to add index to my list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_char = dict((i,c) for i,c in enumerate(chars))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1148003\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "#We set that we're going to send 39 characters and it's gping to predict the character 40\n",
    "step = 5\n",
    "\n",
    "sentences = [] # X inputs\n",
    "next_chars = [] # Y predictions\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print('sequences:',len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148010.6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)/5 # we decide to say that each 3 characteres are going to be a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\nProject Gutenberg’s The Complete Works'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "# samples len(sentences)\n",
    "# len(chars) how many unique characteres \n",
    "# maxlen \n",
    "\n",
    "#We're gping to create the arrays with zeros ( it's transformed to False with type bool)  with the correct size that we need it.\n",
    "# x: (1925776, 40, 114)\n",
    "# y: (1925776, 114)\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    \n",
    "    for t, char in enumerate(sentence):\n",
    "        # then we're going to poblate the array checking if in the posistion i is one of the chars\n",
    "\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1148003, 40, 107)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1148003, 107)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True, ..., False, False, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 107)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0][77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) #number of neurons is 128 and it was chosen arbitrarily\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1148003 samples\n",
      "Epoch 1/2\n",
      "1147648/1148003 [============================>.] - ETA: 0s - loss: 1.7186\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR\"\n",
      "irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR.\n",
      "What the will well all the more and the worsh,\n",
      "    That the compent the stand of the compess,\n",
      "                                                                                                                                                                                                                                                                                                             \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR\"\n",
      "irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR.\n",
      "Whom the arm the wing the quick my guiltes,\n",
      "    But to me streed and dounts,\n",
      "                                                                                                Exit EDRENER Canter the man dount.\n",
      "  LUCIO. Suck sweet her be bear.\n",
      "                                         Exit PRINCE SERCENE\n",
      "\n",
      "\n",
      "                                                                                      \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR\"\n",
      "irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR.\n",
      "Welt: curse with such tooths,\n",
      "    Looker here Cobsel it hath me kill give make,\n",
      "With sick of the mondy sweld and beginingly\n",
      "That wrond and let's presence on, it.\n",
      "\n",
      "PARDALUS.\n",
      "What's swerreas forth of whopely, much gopt,\n",
      "The Embere have gone you us amains and, in\n",
      "    If in his astable, haw she was good.\n",
      "  Being you undinglut’d very Lord striggt.\n",
      "\n",
      ", though scounteble pare poor that ranis\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR\"\n",
      "irit flew. Answer me, heavens.\n",
      "\n",
      "HECTOR.\n",
      "Where\n",
      "l heny when tpy touth me.\n",
      "\n",
      "EULBE.\n",
      "Spy hear he apperies b one an read,\n",
      "If the compentLclutorry stand! Parhow’d scornaising,\n",
      "    \n",
      "Her’ So other to ag’t campss\n",
      "DidKess ithooking o!\n",
      "  cannor tork'd pary’t roy.\n",
      " TMOshould sweet Measso\n",
      "  y never duef firyly, sfeed in dothte\n",
      "at bostlean ourly!\n",
      "Says dirogly, him. Dohg Irhy, tom\n",
      "Sines.\n",
      "\n",
      "[bridiance-\n",
      "    TICRICElSr taken Ala, oury\n",
      "\n",
      "1148003/1148003 [==============================] - 155s 135us/sample - loss: 1.7186\n",
      "Epoch 2/2\n",
      "1147520/1148003 [============================>.] - ETA: 0s - loss: 1.5493\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"hatch’d, and shall be so: Tranio, at onc\"\n",
      "hatch’d, and shall be so: Tranio, at once the seet of the sears and the hearts and so an the more his than the more.\n",
      "    That shall be do good the serves the worsher\n",
      "    That the more of his love the mounters,\n",
      "    And the seal and the son and the heart of the more.\n",
      "    That he shall see the son the sears and so the sound\n",
      "    That a bones the seal the seal of the mind\n",
      "    That the more than the more with her heart,\n",
      "    And the str\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hatch’d, and shall be so: Tranio, at onc\"\n",
      "hatch’d, and shall be so: Tranio, at once contrive thee,\n",
      "    My lord, he shall be sturders of a man that the light possion\n",
      "    Than the mooning and a malling heart of her heart,\n",
      "And thou that the man with a business to know you will he yours\n",
      "                                                                                                                                                                                                  \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"hatch’d, and shall be so: Tranio, at onc\"\n",
      "hatch’d, and shall be so: Tranio, at once him not.\n",
      "Who, how grand at one.\n",
      "\n",
      "DROMIOO.\n",
      "Empessing player?\n",
      "    Awain consest'rable lamiors, this, in,\n",
      "    know not the liporous a gods, if my deptun;\n",
      "    Nor the duchieve with the more upand his hendance\n",
      "Than be one in accerd her someting thee.\n",
      "\n",
      "Whatch grace pain himself, now, your husband doth.\n",
      "\n",
      "PETREcES.\n",
      "None at his faeso, of his sel?\n",
      "  FLADOLK. But, hear 'till knows, and is as \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"hatch’d, and shall be so: Tranio, at onc\"\n",
      "hatch’d, and shall be so: Tranio, at once Pleshid, Caidon.\n",
      "\n",
      "ARSHY.\n",
      "My; oy might besoles sout, gone thing, I am into seve not Diunch:\n",
      "For me straights marking cortion spurn’d they\n",
      "    other. Haff this withuriKjur. But Howness,\n",
      "      ere!\n",
      "\n",
      "CLOWN.\n",
      "Mhinds, thou nows thou’llow'd!\n",
      "Of the guilt? All ham; wastany strong?\n",
      "  CADAN. Come! These insvenless abrogin of yours\n",
      "As ore everiplessor but we empring.\n",
      "\n",
      "FEarcertatio, palace, tha\n",
      "1148003/1148003 [==============================] - 152s 132us/sample - loss: 1.5493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9870211908>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
